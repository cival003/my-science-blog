[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Simone, a senior economist focused on ad-measurement science.\nThis site collects short explainers, code-backed notes, and longer posts I publish publicly."
  },
  {
    "objectID": "posts/2025-07-13-rct-standard-errors/index.html",
    "href": "posts/2025-07-13-rct-standard-errors/index.html",
    "title": "RCT Standard Errors",
    "section": "",
    "text": "Two-arm RCT with \\(n_T\\) treated and \\(n_C\\) control units.\n\nLet \\(\\bar Y_T,\\; s_T^{2}\\) and \\(\\bar Y_C,\\; s_C^{2}\\) denote sample means and variances.\n\nThe estimated effect is \\(\\hat{\\Delta} = \\bar Y_T - \\bar Y_C .\\)\n\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.utils import resample\n\nnp.random.seed(42)\nn = 200\nn_T = n_C = n // 2\n# Simulate covariate and outcome\ndf = pd.DataFrame({\n    'T': np.repeat([1, 0], [n_T, n_C]),\n    'age': np.random.normal(50, 10, n),\n    'preY': np.random.normal(0, 1, n)\n})\ndf['Y'] = 2 + 1.5 * df['T'] + 0.5 * df['age'] + 0.8 * df['preY'] + np.random.normal(0, 2, n)\ndf['school_id'] = np.random.choice([1,2,3,4], size=n)  # for clustering\n\n\nY_T = df.loc[df['T']==1, 'Y']\nY_C = df.loc[df['T']==0, 'Y']\nmean_T = Y_T.mean()\nmean_C = Y_C.mean()\nvar_T = Y_T.var(ddof=1)\nvar_C = Y_C.var(ddof=1)\ndelta_hat = mean_T - mean_C\nprint(f\"Mean (Treated): {mean_T:.3f}\")\nprint(f\"Mean (Control): {mean_C:.3f}\")\nprint(f\"Variance (Treated): {var_T:.3f}\")\nprint(f\"Variance (Control): {var_C:.3f}\")\nprint(f\"Estimated Effect (Delta): {delta_hat:.3f}\")\n\nMean (Treated): 27.921\nMean (Control): 26.966\nVariance (Treated): 24.378\nVariance (Control): 24.434\nEstimated Effect (Delta): 0.954"
  },
  {
    "objectID": "posts/2025-07-13-rct-standard-errors/index.html#setup",
    "href": "posts/2025-07-13-rct-standard-errors/index.html#setup",
    "title": "RCT Standard Errors",
    "section": "",
    "text": "Two-arm RCT with \\(n_T\\) treated and \\(n_C\\) control units.\n\nLet \\(\\bar Y_T,\\; s_T^{2}\\) and \\(\\bar Y_C,\\; s_C^{2}\\) denote sample means and variances.\n\nThe estimated effect is \\(\\hat{\\Delta} = \\bar Y_T - \\bar Y_C .\\)\n\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.utils import resample\n\nnp.random.seed(42)\nn = 200\nn_T = n_C = n // 2\n# Simulate covariate and outcome\ndf = pd.DataFrame({\n    'T': np.repeat([1, 0], [n_T, n_C]),\n    'age': np.random.normal(50, 10, n),\n    'preY': np.random.normal(0, 1, n)\n})\ndf['Y'] = 2 + 1.5 * df['T'] + 0.5 * df['age'] + 0.8 * df['preY'] + np.random.normal(0, 2, n)\ndf['school_id'] = np.random.choice([1,2,3,4], size=n)  # for clustering\n\n\nY_T = df.loc[df['T']==1, 'Y']\nY_C = df.loc[df['T']==0, 'Y']\nmean_T = Y_T.mean()\nmean_C = Y_C.mean()\nvar_T = Y_T.var(ddof=1)\nvar_C = Y_C.var(ddof=1)\ndelta_hat = mean_T - mean_C\nprint(f\"Mean (Treated): {mean_T:.3f}\")\nprint(f\"Mean (Control): {mean_C:.3f}\")\nprint(f\"Variance (Treated): {var_T:.3f}\")\nprint(f\"Variance (Control): {var_C:.3f}\")\nprint(f\"Estimated Effect (Delta): {delta_hat:.3f}\")\n\nMean (Treated): 27.921\nMean (Control): 26.966\nVariance (Treated): 24.378\nVariance (Control): 24.434\nEstimated Effect (Delta): 0.954"
  },
  {
    "objectID": "posts/2025-07-13-rct-standard-errors/index.html#neyman-analytical-standard-error",
    "href": "posts/2025-07-13-rct-standard-errors/index.html#neyman-analytical-standard-error",
    "title": "RCT Standard Errors",
    "section": "2 Neyman analytical standard error",
    "text": "2 Neyman analytical standard error\n\\[\n\\widehat{\\operatorname{Var}}_{\\text{Neyman}}\\!\\bigl(\\hat{\\Delta}\\bigr)\n  = \\frac{s_T^{2}}{n_T} + \\frac{s_C^{2}}{n_C}.\n\\]\n\nExact under simple random assignment. If the assignment is not simple random (e.g., clustered, stratified, or otherwise dependent), this formula may underestimate or misestimate the true variance.\nIgnores covariates; best for large, clean designs.\nIn smaller samples, or if covariates are strongly related to the outcome, adjusting for covariates (e.g., via regression) can increase precision (reduce standard errors).\n\n\nY_T = df.loc[df['T']==1, 'Y']\nY_C = df.loc[df['T']==0, 'Y']\ns2_T = Y_T.var(ddof=1)\ns2_C = Y_C.var(ddof=1)\nse_neyman = np.sqrt(s2_T/n_T + s2_C/n_C)\nprint(f\"Neyman SE: {se_neyman:.3f}\")\n\nNeyman SE: 0.699"
  },
  {
    "objectID": "posts/2025-07-13-rct-standard-errors/index.html#ols-with-hc2hc3-robust",
    "href": "posts/2025-07-13-rct-standard-errors/index.html#ols-with-hc2hc3-robust",
    "title": "RCT Standard Errors",
    "section": "3 OLS with HC2/HC3 (“robust”)",
    "text": "3 OLS with HC2/HC3 (“robust”)\nRun OLS of \\(Y\\) on treatment \\(T\\) and any baseline covariates \\(X\\); report HC3 SEs.\n\nX = sm.add_constant(df[['T', 'age', 'preY']])\nmodel = sm.OLS(df['Y'], X).fit(cov_type='HC3')\nse_hc3 = model.bse['T']\nprint(f\"OLS HC3 SE: {se_hc3:.3f}\")\n\nOLS HC3 SE: 0.284\n\n\n\nHC3 handles heteroskedasticity.\nEquals Neyman when the model is saturated (the regression only includes treatment (no covariates), robust SE = Neyman SE.)\n\nTo show this second point, we fit a regression of \\(Y\\) on \\(T\\) only (no covariates), compute the robust (HC3) standard error for the treatment effect, and compare it to the Neyman standard error. The results below confirm that the two are equal:\n\n\nOLS HC3 SE (T only): 0.702177\nNeyman SE: 0.698658\nDifference: 3.519708e-03"
  },
  {
    "objectID": "posts/2025-07-13-rct-standard-errors/index.html#cluster-robust-standard-errors",
    "href": "posts/2025-07-13-rct-standard-errors/index.html#cluster-robust-standard-errors",
    "title": "RCT Standard Errors",
    "section": "4 Cluster-robust standard errors",
    "text": "4 Cluster-robust standard errors\nNeeded when assignment or outcomes cluster (e.g., classrooms, markets).\nIn terms of the model, clustering means that the error terms are correlated within groups (clusters):\n\\[\n\\operatorname{Cov}(\\epsilon_{ij},\\; \\epsilon_{i'j}) &gt; 0\n\\quad \\text{for} \\quad i \\neq i', \\text{ within the same cluster } j,\n\\]\nThat is, individuals in the same cluster share unobserved factors, so their errors are not independent.\nCluster-robust standard errors are usually larger than conventional (non-clustered) standard errors. When there is strong intra-cluster correlation, the estimated SEs can be much larger, reflecting the reduced effective sample size due to the similarity of outcomes within clusters.\n\nmodel_cluster = sm.OLS(df['Y'], X).fit(cov_type='cluster', cov_kwds={'groups': df['school_id']})\nse_cluster = model_cluster.bse['T']\nprint(f\"Cluster-robust SE: {se_cluster:.3f}\")\n\nCluster-robust SE: 0.304"
  },
  {
    "objectID": "posts/2025-07-13-rct-standard-errors/index.html#bootstrap",
    "href": "posts/2025-07-13-rct-standard-errors/index.html#bootstrap",
    "title": "RCT Standard Errors",
    "section": "5 Bootstrap",
    "text": "5 Bootstrap\n\n5.1 Simple bootstrap\nResample individuals with replacement; compute \\(\\hat{\\Delta}\\) each time; use the empirical standard deviation.\n\n# Bootstrap for regression SE\nboot_ests_reg = []\n# Bootstrap for simple difference in means\nboot_ests_diff = []\nfor _ in range(1000):\n    boot = resample(df)\n    # Regression bootstrap\n    Xb = sm.add_constant(boot[['T', 'age', 'preY']])\n    mb = sm.OLS(boot['Y'], Xb).fit()\n    boot_ests_reg.append(mb.params['T'])\n    # Simple difference in means bootstrap\n    Y_Tb = boot.loc[boot['T']==1, 'Y']\n    Y_Cb = boot.loc[boot['T']==0, 'Y']\n    boot_ests_diff.append(Y_Tb.mean() - Y_Cb.mean())\nse_boot_reg = np.std(boot_ests_reg, ddof=1)\nse_boot_diff = np.std(boot_ests_diff, ddof=1)\nprint(f\"Bootstrap SE (regression): {se_boot_reg:.3f}\")\nprint(f\"Bootstrap SE (difference in means): {se_boot_diff:.3f}\")\n\nBootstrap SE (regression): 0.284\nBootstrap SE (difference in means): 0.692\n\n\n\n\n5.2 Cluster bootstrap\nResample clusters to respect dependence.\n\nn_boot = 1000\nboot_ests_cluster_reg = []  # Regression-based\nboot_ests_cluster_diff = []  # Difference in means\nclusters = df['school_id'].unique()\nfor _ in range(n_boot):\n    sampled_clusters = np.random.choice(clusters, size=len(clusters), replace=True)\n    boot = pd.concat([df[df['school_id'] == cid] for cid in sampled_clusters], ignore_index=True)\n    # Regression-based cluster bootstrap\n    Xb = sm.add_constant(boot[['T', 'age', 'preY']])\n    mb = sm.OLS(boot['Y'], Xb).fit()\n    boot_ests_cluster_reg.append(mb.params['T'])\n    # Difference in means cluster bootstrap\n    Y_Tb = boot.loc[boot['T'] == 1, 'Y']\n    Y_Cb = boot.loc[boot['T'] == 0, 'Y']\n    if len(Y_Tb) &gt; 0 and len(Y_Cb) &gt; 0:\n        boot_ests_cluster_diff.append(Y_Tb.mean() - Y_Cb.mean())\nse_boot_cluster_reg = np.std(boot_ests_cluster_reg, ddof=1)\nse_boot_cluster_diff = np.std(boot_ests_cluster_diff, ddof=1)\nprint(f\"Cluster bootstrap SE (regression): {se_boot_cluster_reg:.3f}\")\nprint(f\"Cluster bootstrap SE (difference in means): {se_boot_cluster_diff:.3f}\")\n\nCluster bootstrap SE (regression): 0.259\nCluster bootstrap SE (difference in means): 0.405"
  },
  {
    "objectID": "posts/2025-07-13-rct-standard-errors/index.html#randomisation-inference",
    "href": "posts/2025-07-13-rct-standard-errors/index.html#randomisation-inference",
    "title": "RCT Standard Errors",
    "section": "6 Randomisation inference",
    "text": "6 Randomisation inference\nRandomisation inference is a nonparametric method that uses the known assignment mechanism of an experiment to test hypotheses. It is exact under the randomisation used, making no assumptions about outcome distributions.\nUnder the null hypothesis (no treatment effect), outcomes would be equally likely under any possible assignment of treatment labels. Shuffle (permute) the treatment labels many times, each time calculating the test statistic (e.g., difference in means). The p-value is the proportion of shuffled assignments where the statistic is as extreme or more extreme than observed.\nThis method provides exact p-values, even in small samples or with unusual outcome distributions. It is useful for non-standard estimands (like medians) or complex assignment mechanisms.\n\n# Randomisation inference example: calculate p-value for treatment effect\nnp.random.seed(42)\nreps = 5000\ny = df['Y'].values\nT = df['T'].values\nobs = np.mean(y[T==1]) - np.mean(y[T==0])\nmore_extreme = 0\nfor _ in range(reps):\n    shuffled = np.random.permutation(T)\n    diff = np.mean(y[shuffled==1]) - np.mean(y[shuffled==0])\n    if abs(diff) &gt;= abs(obs):\n        more_extreme += 1\npval = more_extreme / reps\nprint(f\"Randomisation inference p-value: {pval:.4f}\")\n\nRandomisation inference p-value: 0.1752"
  },
  {
    "objectID": "posts/2025-07-13-rct-standard-errors/index.html#choosing-among-methods",
    "href": "posts/2025-07-13-rct-standard-errors/index.html#choosing-among-methods",
    "title": "RCT Standard Errors",
    "section": "7 Choosing among methods",
    "text": "7 Choosing among methods\n\n\n\n\n\n\n\nSituation\nPreferred SE method\n\n\n\n\nLarge i.i.d. RCT\nNeyman or HC3\n\n\nHeteroskedastic outcomes\nHC3\n\n\nClustered assignment or outcomes\nCluster-robust\n\n\nSmall samples\nRandomisation inference\n\n\nNon-analytic estimators (e.g., quantiles)\nBootstrap"
  },
  {
    "objectID": "posts/2025-07-13-rct-standard-errors/index.html#references",
    "href": "posts/2025-07-13-rct-standard-errors/index.html#references",
    "title": "RCT Standard Errors",
    "section": "8 References",
    "text": "8 References\n\nFreedman (2008) “On regression adjustments in experiments.”\n\nImbens & Rubin (2015) Causal Inference.\n\nAbadie, Athey, Imbens & Wooldridge (2023) “Sampling- vs design-based inference in experiments.”"
  },
  {
    "objectID": "posts/2025-07-10-test/index.html",
    "href": "posts/2025-07-10-test/index.html",
    "title": "Test Page",
    "section": "",
    "text": "Test page including equations: \\[\n\\hat{\\Delta} = \\bar Y_T - \\bar Y_C .\n\\]"
  },
  {
    "objectID": "posts/2025-07-10-test/index.html#equations",
    "href": "posts/2025-07-10-test/index.html#equations",
    "title": "Test Page",
    "section": "",
    "text": "Test page including equations: \\[\n\\hat{\\Delta} = \\bar Y_T - \\bar Y_C .\n\\]"
  },
  {
    "objectID": "posts/2025-07-10-test/index.html#markdows",
    "href": "posts/2025-07-10-test/index.html#markdows",
    "title": "Test Page",
    "section": "2 Markdows",
    "text": "2 Markdows\nimport statsmodels.api as sm\nX = sm.add_constant(df[[\"T\", \"age\", \"preY\"]])\nmodel = sm.OLS(df[\"Y\"], X).fit(cov_type=\"HC3\")\nmodel.summary()"
  },
  {
    "objectID": "posts/2025-07-10-test/index.html#code-rendering",
    "href": "posts/2025-07-10-test/index.html#code-rendering",
    "title": "Test Page",
    "section": "3 Code Rendering",
    "text": "3 Code Rendering\n\nimport numpy as np\nimport pandas as pd\nx = np.random.normal(loc=0.0, scale=1.0, size=1000)\ny = np.mean(x)\nprint(y)\n\n-0.08205250312560684\n\n\n\nprint(np.var(x))\n\n1.0108573440815711"
  },
  {
    "objectID": "posts/2025-07-10-test/index.html#table",
    "href": "posts/2025-07-10-test/index.html#table",
    "title": "Test Page",
    "section": "4 Table",
    "text": "4 Table\n\n\n\nTable 1: Table Title\n\n\n\n\n\n\n\n(a) First Panel\n\n\n\n\n\nCol1\nCol2\nCol3\n\n\n\n\nA\nB\nC\n\n\nE\nF\nG\n\n\nA\nG\nG\n\n\n\n\n\n\n\n\n\n\n\n(b) Second Panel\n\n\n\n\n\nCol1\nCol2\nCol3\n\n\n\n\nA\nB\nC\n\n\nE\nF\nG\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2\n\n\n\n\n\nCol1\nCol2\nCol3\n\n\n\n\nA\nB\nC\n\n\nE\nF\nG\n\n\nA\nG\nG\n\n\n\n\n\n\nSee Table 1 for details, especially Table 1 (b)."
  },
  {
    "objectID": "posts/2025-07-10-test/index.html#plot-with-code",
    "href": "posts/2025-07-10-test/index.html#plot-with-code",
    "title": "Test Page",
    "section": "5 Plot with Code",
    "text": "5 Plot with Code\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nmpg = sns.load_dataset(\"mpg\").dropna()\nsns.scatterplot(\n    data=mpg,\n    x=\"horsepower\",\n    y=\"mpg\",\n    hue=\"origin\",\n    s=40,\n    alpha=0.8,\n    palette=\"Dark2\"\n)\nplt.xlabel(\"Horsepower\")\nplt.ylabel(\"Miles per gallon\")\nplt.title(\"Fuel efficiency vs. horsepower\")\nplt.tight_layout()\n\n\n\nFuel efficiency vs. horsepower by origin (with code)"
  },
  {
    "objectID": "posts/2025-07-10-test/index.html#plot-without-source-code",
    "href": "posts/2025-07-10-test/index.html#plot-without-source-code",
    "title": "Test Page",
    "section": "6 Plot without Source Code",
    "text": "6 Plot without Source Code\n\n\n\n\n\nFigure 1: Fuel efficiency vs. horsepower by origin"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes",
    "section": "",
    "text": "RCT Standard Errors\n\n\n\nRCT\n\nStandard Errors\n\n\n\nPractical guide to choosing and interpreting standard error methods in RCTs: Neyman, HC3, cluster-robust, bootstrap, and randomisation inference. Includes code, guidance, and when to use each approach.\n\n\n\n\n\nJul 13, 2025\n\n\nSimone\n\n\n\n\n\n\n\n\n\n\n\n\nTest Page\n\n\n\nTest\n\n\n\nTest Page\n\n\n\n\n\nJul 10, 2025\n\n\nSimone\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-07-13-rct-standard-errors/index.html#summary-of-standard-error-and-inference-methods",
    "href": "posts/2025-07-13-rct-standard-errors/index.html#summary-of-standard-error-and-inference-methods",
    "title": "RCT Standard Errors",
    "section": "8 Summary of Standard Error and Inference Methods",
    "text": "8 Summary of Standard Error and Inference Methods\nBelow is a summary of the main methods for estimating standard errors or p-values in RCT analysis, with their calculated values (from the code above) and guidance on when to use each. After running the code, copy the printed results into the table below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nWhen to Use\n\n\n\n\nNeyman analytical SE (diff means)\nClassic, large RCTs with simple randomisation; no covariates; outcomes i.i.d.\n\n\nOLS HC3 robust SE (regression)\nAny RCT with covariates; robust to heteroskedasticity; default for most regression analyses\n\n\nOLS Cluster-robust SE (regression)\nAssignment or outcomes clustered (e.g., by school, clinic); regression with covariates\n\n\nBootstrap SE (diff means)\nSmall samples; non-standard estimands (e.g., medians); minimal assumptions\n\n\nBootstrap SE (regression)\nSmall samples; regression with covariates; non-standard estimands; minimal assumptions\n\n\nCluster bootstrap SE (diff means)\nClustered data; small samples; non-standard estimands; minimal assumptions\n\n\nCluster bootstrap SE (regression)\nClustered data; regression with covariates; robust to cluster structure and small samples"
  },
  {
    "objectID": "posts/2025-07-13-rct-standard-errors/index.html#bayesian-estimates-of-standard-error",
    "href": "posts/2025-07-13-rct-standard-errors/index.html#bayesian-estimates-of-standard-error",
    "title": "RCT Standard Errors",
    "section": "7 Bayesian Estimates of Standard Error",
    "text": "7 Bayesian Estimates of Standard Error\nBayesian methods provide an alternative approach to uncertainty quantification in RCTs by estimating the full posterior distribution of treatment effects, rather than relying solely on standard errors from frequentist estimators. In the Bayesian framework, uncertainty is typically summarized using credible intervals (e.g., the 95% credible interval), which represent the range within which the true effect lies with a specified probability, given the data and prior.\nHow it works: - Specify a likelihood for the data and a prior distribution for the parameters (e.g., treatment effect). - Use Bayes’ theorem to compute the posterior distribution of the treatment effect. - The standard deviation of the posterior distribution can be interpreted as a Bayesian analogue to the standard error. - The 95% credible interval is often reported instead of (or alongside) a standard error.\nWhen to use: - When you want to incorporate prior information or beliefs about the treatment effect. - For small samples, complex models, or when the likelihood is not well-approximated by standard frequentist assumptions. - When you want a direct probabilistic interpretation of uncertainty (e.g., “there is a 95% probability the effect lies in this interval”).\nExample (conceptual): - Fit a Bayesian regression model (e.g., using PyMC or Stan) for the treatment effect. - Summarize the posterior with its mean and standard deviation (Bayesian SE), and report the 95% credible interval.\nNote: - Bayesian credible intervals and standard errors depend on the choice of prior and model specification. - In large samples with non-informative priors, Bayesian and frequentist intervals often agree."
  }
]